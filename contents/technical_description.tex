% Technical description of the method to solve the task (~1 - 2 page)

\section{Approach Description}

% NSpM
Our basis approach is the Neural SPARQL Machine (NSpM)\cite{soru-marx-nampi2018},
which considers SPARQL query as another natural language and employs machine translation techniques. 
NSpM consists of three main components: 
\textit{Generator}, \textit{Learner} and \textit{Interpreter}. 
\textit{Generator} generates a training set
by fulling entities from knowledge graph into question and query templates with placeholders. 
Following is an example of template and generated questions and queries:

\begin{verbatim}
    Where is <A> located in?; 
                    SELECT ?x { <A> dbo:location ?x }
\end{verbatim}

\begin{verbatim}
    Where is London located in?; 
                    SELECT ?x { dbr:London dbo:location ?x }
    Where is the Colosseum located in?; 
                    SELECT ?x { dbr:Colosseum dbo:location ?x }
    Where is Mount Everest located in?; 
                    SELECT ?x { dbr:Mount_Everest dbo:location ?x }
\end{verbatim}

\textit{Learner} is a deep neural network based translator, 
which translates a sequence of tokens in natural language
into a sequence which encodes a SPARQL query after training.
\textit{Interpreter} uses the learned model to predict a SPARQL query for received question.

There are two drawbacks in this original NSpM.
First, since training data is generated using templates, 
the number of entities are severely restricted,
which leads to a bad performance in evaluation.
Or the training set has to be very large
that requires lots of computational resources. 
Besides, there is no template for QALD 8 and 9 training set,
which make comparisons to other QA systems difficult. 
On the other hand, 
learner consists only of a one-layer encoder and a one-layer decoder.
The performance will be greatly improved using other translation model.
To improve NSpM, we used DBpedia Spotlight and Pegasus model. 

% DBpedia spotlight
DBpedia Spotlight\cite{isem2013daiber} is a tool for automatic annotating and entity linking, 
which performs entity extraction including entity detection and name resolution. 
First, we integrated DBpedia Spotlight into the interpreter to detect entities. 
We abandoned generator and trained with templates directly. 
Then in the prediction phase, 
when interpreter receives a question, 
DBpedia Spotlight detects entities in the question,
then for each annotated entity, 
interpreter stores the DBpedia entity with the highest confidence score 
and replaces it with placeholders. 
Interpreter translates this modified question into a SPARQL query with placeholders
and restores entities again. 
Integration of DBpedia Spotlight in interpreter decreases the size of training set
and therefore the training effort is reduced. 
However, the performance is better,
since it uses the entire knowledge graph while prediction
and learner only needs to focus on learning predicates in SPARQL queries. 
DBpedia Spotlight is also used for generate templates from QALD 8 and 9 training set
by finding entities in quesitons and queries and replacing them with placeholders. 

% Pegasus: pre-training with extracted gap-sentences for abstractive summarization
Pegasus\cite{10.5555/3524938.3525989} is a pre-trained model with extracted gap-sentences for abstractive summarization. 
We used hugging face pegasus model, which is pre-trained on a large text corpora. 
Pegasus model enlarges our set of tokens and improves the quality of translation. 
Moreover, we pre-trained again on LC-QALD dataset and fine-tuned on QALD 8 and 9 to get better result.

We took the basic idea of NSpM to consider SPARQL as another natural language
and modified the components of NSpM:
giving up \textit{generator},
using Pegasus model for learning,
and integrate DBpedia Spotlight into \textit{Interpreter}.

% ConvS2S
We also tried Convolutional Sequence-to-Sequence model (ConvS2S)\cite{DBLP:journals/corr/GehringAGYD17} to improve the translation quality.
ConvS2S converts an input sequence to a output sequence using convolution neural networks. 
This approach did not work in the end
as we encountered too many troubles while reimplementing it with tensorflow 2. 

% Gerbil
For evaluation we used gerbil\cite{gerbil}.
Gerbil is a web-based platform for evaluation and comparison of QA systems.
User can upload test set and add a QA system via URI or upload a JSON file with answers in QALD-JSON format. 
We focused on QALD 8 and 9 for evaluation. 
At the beginning, we generated a JSON file with answers and uploaded it to gerbil. 
Later once we deployed our QA system on VM, 
we added our system via URI in every evaluation. 

% VM and website