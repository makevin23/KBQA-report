% Encoding: UTF-8

@article{DBLP:journals/corr/abs-2103-06752,
  author    = {Daniel Vollmers and
               Rricha Jalota and
               Diego Moussallem and
               Hardik Topiwala and
               Axel{-}Cyrille Ngonga Ngomo and
               Ricardo Usbeck},
  title     = {Knowledge Graph Question Answering using Graph-Pattern Isomorphism},
  journal   = {CoRR},
  volume    = {abs/2103.06752},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.06752},
  eprinttype = {arXiv},
  eprint    = {2103.06752},
  timestamp = {Tue, 16 Mar 2021 11:26:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-06752.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{soru-marx-nampi2018,
    author = "Tommaso Soru and Edgard Marx and Andr\'e Valdestilhas and Diego Esteves and Diego Moussallem and Gustavo Publio",
    title = "Neural Machine Translation for Query Construction and Composition",
    year = "2018",
    journal = "ICML Workshop on Neural Abstract Machines \& Program Induction (NAMPI v2)",
    url = "https://arxiv.org/abs/1806.10478",
}

@inproceedings{gerbil,
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano},
title = {A Framework for Benchmarking Entity-Annotation Systems},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488411},
doi = {10.1145/2488388.2488411},
abstract = {In this paper we design and implement a benchmarking framework for fair and exhaustive comparison of entity-annotation systems. The framework is based upon the definition of a set of problems related to the entity-annotation task, a set of measures to evaluate systems performance, and a systematic comparative evaluation involving all publicly available datasets, containing texts of various types such as news, tweets and Web pages. Our framework is easily-extensible with novel entity annotators, datasets and evaluation measures for comparing systems, and it has been released to the public as open source. We use this framework to perform the first extensive comparison among all available entity annotators over all available datasets, and draw many interesting conclusions upon their efficiency and effectiveness. We also draw conclusions between academic versus commercial annotators.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {249â€“260},
numpages = {12},
keywords = {wikipedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{isem2013daiber,
  title = {Improving Efficiency and Accuracy in Multilingual Entity Extraction},
  author = {Joachim Daiber and Max Jakob and Chris Hokamp and Pablo N. Mendes},
  year = {2013},
  booktitle = {Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)}
}

@inproceedings{10.5555/3524938.3525989,
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
title = {PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pretraining large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1051},
numpages = {12},
series = {ICML'20}
}

@article{DBLP:journals/corr/GehringAGYD17,
  author    = {Jonas Gehring and
               Michael Auli and
               David Grangier and
               Denis Yarats and
               Yann N. Dauphin},
  title     = {Convolutional Sequence to Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1705.03122},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03122},
  eprinttype = {arXiv},
  eprint    = {1705.03122},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GehringAGYD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Comment{jabref-meta: databaseType:bibtex;}
